当然，下面分别阐述Proximal Gradient Descent（PGD）、Subgradient Method、Alternating Direction Method of Multipliers (ADMM)、和Fast Iterative Shrinkage-Thresholding Algorithm (FISTA)的算法流程设计。

### Proximal Gradient Descent (PGD)

PGD适用于优化形式为$f(x) + g(x)$的问题，其中$f(x)$是光滑的（具有Lipschitz连续梯度），而$g(x)$是凸的但可能非光滑的。

#### 算法流程设计：

1. 初始化：选取初始点$x_0$和步长参数$\alpha > 0$。

2. 对于每一步$k = 1, 2, 3, ...$

   a. 计算梯度：$\nabla f(x_k)$。

   b. 进行梯度下降和近似操作：$$x_{k+1} = \arg\min_x \left\{ g(x) + \langle \nabla f(x_k), x - x_k \rangle + \frac{1}{2\alpha}\|x - x_k\|^2 \right\}$$。

   c. 更新$x_k$为$x_{k+1}$。

   d. 检查收敛条件，如果不满足则继续迭代。

### Subgradient Method

Subgradient Method适用于非光滑但凸的优化问题。

#### 算法流程设计：

1. 初始化：选取初始点$x_0$和步长序列$\alpha_k > 0$。

2. 对于每一步$k = 1, 2, 3, ...$

   a. 计算$f(x_k)$的一个次梯度$\partial f(x_k)$。

   b. 更新点：$x_{k+1} = x_k - \alpha_k \cdot s_k$，其中$s_k \in \partial f(x_k)$。

   c. 检查收敛条件，如果不满足则继续迭代。

### Alternating Direction Method of Multipliers (ADMM)

ADMM适用于分解型优化问题，形式为$\min_x f(x) + g(z)$ s.t. $Ax + Bz = c$。

#### 算法流程设计：

1. 初始化：选取初始点$x_0, z_0$和拉格朗日乘子$u_0$，以及惩罚参数$\rho > 0$。

2. 对于每一步$k = 1, 2, 3, ...$

   a. 更新$x$：$\hat{x}_{k+1} = \arg\min_x \left\{ f(x) + (\rho/2)\|Ax + Bz_k - c + u_k\|^2 \right\}$。

   b. 更新$z$：$\hat{z}_{k+1} = \arg\min_z \left\{ g(z) + (\rho/2)\|A\hat{x}_{k+1} + Bz - c + u_k\|^2 \right\}$。

   c. 更新拉格朗日乘子：$u_{k+1} = u_k + \rho(A\hat{x}_{k+1} + B\hat{z}_{k+1} - c)$。

   d. 检查收敛条件，如果不满足则继续迭代。

### Fast Iterative Shrinkage-Thresholding Algorithm (FISTA)

FISTA是PGD的一个加速版本，用于解决形式为$min_x f(x) + g(x)$ $的问题，其中$$f(x)$是光滑的，而$g(x)$是非光滑的。

#### 算法流程设计：

1. 初始化：选取初始点$x_0$，以及步长参数$\alpha > 0$和Lipschitz常数$L$。

2. 设置$y_0 = x_0$和$t_0 = 1$。

3. 对于每一步$k = 1, 2, 3, ...$

   a. 计算梯度：$\nabla f(y_k)$。

   b. 近似操作：$x_{k+1} = \arg\min_x \left\{ g(x) + \langle \nabla f(y_k), x - y_k \rangle + \frac{1}{2\alpha}\|x - y_k\|^2 \right\}$。

   c. 更新$y$：$y_{k+1} = x_{k+1} + \left( \frac{t_k - 1}{t_{k+1}} \right)(x_{k+1} - x_k)$，其中$t_{k+1} = \frac{1 + \sqrt{1 + 4t_k^2}}{2}$。

   d. 检查收敛条件，如果不满足则继续迭代。

